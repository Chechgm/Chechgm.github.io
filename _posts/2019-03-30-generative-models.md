---
layout: post
title:  Generative models
date: 30-03-2019
img_link: /assets/2_plated_graph.png
excerpt: This post introduces some of the basic concepts necessary for the following posts and the research I work on. The aim of the post is to introduce you to generative modeling and some of its terminology. Future posts will deepen on how to learn and use these models.
---
## Two probability equations
In this tutorial, we are going to use two probability equations that are widely used in generative models: the Bayes theorem and the marginalization rule. Let's begin with the Bayes theorem:

$$
p(y \lvert x) = \frac{p(y, x)}{p(x)} = \frac{p(x \lvert y)p(y)}{p(x)} \qquad \text{Bayes theorem}
$$

In the Bayes theorem, $$p(y\lvert x)$$ is called the **"posterior"**, $$p(x\lvert y)$$ is called the **"likelihood"**, $$p(y)$$ is called the **"prior"** and $$p(x)$$ is called the **"evidence"** or the **"partition function"**. These are recurrent terms in probabilistic modeling so it is useful to have them in mind.

Now let's review the marginalization rule:

$$
\left.\begin{align}
	p(x)  = \sum_{y}p(y, x) \\
	p(x)  = \int p(y, x)dy
\end{align}
\right\}
\qquad \text{Marginalization in discrete and continuous variables}
$$

These equations give us the probability distribution of $$x$$ regardless of the values that $$y$$ takes. This, in connection with Bayes theorem, allows us to compute conditional distributions from joint distributions.

## Definition

Generative models are a class of models that aim to infer the distribution of the data across *all* of it's dimensions. Mathematically, if we have a set $$\{x_{i}\}_{i\in m}$$ where $$x_{i} \in \mathbb{R}^{D}$$, and $$m$$ corresponds to the number of observations, the generative model aims to find the joint distribution $$p(\mathbf{x})$$ (or $$p(y, \mathbf{x}$$) if we have labels in our data). This is different from discriminative models which aim to model *conditional* distributions of variables: $$p(y \lvert \mathbf{x})$$. Since the aim of generative models is to represent a distribution, this places them in the family of probabilistic or Bayesian methods.

## What are these models used for?

Generative models are used for different tasks but, compared to discriminative models, they can perform <a href="https://en.wikipedia.org/wiki/Density_estimation" target="_blank"><em>density estimation</em></a>. This means they can return a likelihood value of your data points under your model. A direct application of density estimation is anomaly detection: given a probability distribution $$p(\mathbf{x})$$ how likely is a point $$x$$ to happen?

After modeling the probability distribution, a second major task that generative models can perform is synthetic data *generation*. This means, generating data that resembles (and potentially has the same properties) the data used to train the model. Hence the name "Generative" models. Data generation is, for example, widely used in image synthesis which is essential to make realistic computer games.

<div class="blogViewImg">
  <figure>
    <img src="{{ site.baseurl }}/assets/2_biggan.png" alt="" />
    <figcaption>Images generated by state of the art generative models. <a href="https://arxiv.org/pdf/1809.11096.pdf" target="_blank">(source)</a></figcaption>
  </figure>
</div>

 Other tasks, such as providing a label or an estimate of a variable $$\hat{y}$$ like in discriminative models, can be done through the Bayes theorem and marginalization.

 Generative models are extremely useful to do machine learning without knowledge of specific algorithms, the context in which they are used and the assumptions they make. In contrast, generative modeling is the building block of *model based machine learning* which "only" requires knowledge of probability. Common methods in machine learning and statistics can be represented as generative models given a probabilistic approach. To name a few, <a href="http://www.robots.ox.ac.uk/~cvrg/hilary2006/ppca.pdf" target="_blank">Principal Component Analysis (PCA)</a>, Linear Regression and even the <a href="http://http://mlg.eng.cam.ac.uk/zoubin/papers/vietri.pdf" target="_blank">Kalman Filter!</a>

 Generative models can also be useful to disentangle causality. In fact, Pearl's approach to causality is based on them. For studying extensively generative models and causality, Pearl earned the Turing award in 2011.

## How to build your own generative model?

In order to do generative models, we have to include a simplifying assumption: Variables have parametric distributions . Therefore, using the notation from above, we will model $$p_{\theta}(x)$$. In order to do this, we have to think about relations between the variables we want to model and the variables we observe. This leads us to the next building block of generative models: latent variables.

One key concept to take into account when thinking about the relations between variables is that of "latent" or "hidden" variables. These, as their name suggests, are variables which are not directly observed but we believe to be an essential part of our model. In most modeling scenarios, they represent an abstraction of an element of the theory we want to model. You can find some examples of these latent variables across many sciences. To list a few: in economics, "intelligence" is a variable that is not directly observed (and actually impossible to observe) but that would potentially have a direct effect on salary. In computational linguists (also known as Natural Language Processing), all written documents have a combination of topics which are not always stated and they affect the combination of words used in the document.

After we have made the distributional assumptions of both observed and latent variables and the way they relate based on theory or, actually, pure common sense, we have our generative model. Let's slightly digress into how to represent these models.

## Representation (a short digression into graphical models)
There are three common ways to represent generative models: Directed graphs, undirected graphs and factor graphs. Each of them has strengths and weakness and each of them has a different representation power when it comes to generative models. In this section, I will talk a bit about directed graphs.

In their basic form, directed graphs are composed of nodes and directed edges. The nodes represent either random variables or parameters. The edges represent a conditioning relation between two variables. If one variable (a parent) has an edge pointing to another variable (a child) then the probability of the child depends on the value that the parent variable takes. Note that the type of directed graphs we use in generative models is a subset which are called Directed Acyclical Graphs (DAG) which means that there are not any cycles in the proposed graph structure. (Remember we were talking about representation power before? well, this condition might be a limitant of directed graphs)

Let's do a basic example to check how these concepts work. Suppose you want to model an outcome variable $$x$$ of two individuals. You believe that there are two factors that influence $$x$$: $$\beta$$ which influences both individuals in the same way and $$z_{i}$$ which is specific to each individual. $$\alpha$$ represent some fixed parameter which influences the distribution of $$\beta$$. The latent variable $$\beta$$ would be called a global variable and the latent variable $$z$$ would be called local variable. This simple graphical model can be represented in the following way:

<div class="blogViewImg">
  <figure>
    <img src="{{ site.baseurl }}/assets/2_plated_graph.png" alt="" />
    <figcaption>Plated graphical model</figcaption>
  </figure>

  <figure>
    <img src="{{ site.baseurl }}/assets/2_graph.png" alt="" />
    <figcaption>Graphical model without plate</figcaption>
  </figure>
</div>

Both of these images represent the same model. However, the left introduces the convenient "plate" notation. The plate notation is particularly useful when some parts of the graph are repeated let's say across parameters or across individuals like in our case. The "plate" is just a square enclosing the variables that are repeated. The letter in the bottom right corner of the plate is the number of times these variables are repeated. The last notation I want to introduce is the colors and sizes of the previous graphs. Grey filled circles represent observed variables, white circles represent latent variables and solid black dots represent fixed parameters.

<center><img src="{{ site.baseurl }}/assets/2_graph_legend.png" alt="" /></center>
<center>Graphical model legend</center>

## Back to generative models and generative processes

We have been worrying about how to build generative models and how to represent them with graphs, but we have forgotten that they also represent a probability distribution. Mathematically, the graph introduced in the last section represents an equation:

$$
p(\mathbf{x}, \mathbf{z}, \beta \lvert \alpha) = p(\beta \lvert \alpha)\prod_{i=1}^{M=2}p(z_{i} \lvert \beta)p(x_{i} \lvert z_{i}, \beta)
$$

This distribution, represents the *joint distribution* of our variables conditioned on a single parameter. It is also the likelihood (same likelihood from Bayes theorem) of our data and latent variables given the model, meaning the probability of our data given the model assumptions we made about the distributions and the parameters governing them. This likelihood function (after learning the distributions and the parameters of our model) allows us to evaluate the probability of a certain data point of our data set. This is the *density estimation* I mentioned before on the benefits of generative modeling.

Last, but not least, I would like to introduce the generative process of generative models. This, besides the mathematical form and the graph, is a way to represent generative models. First, let's make some distributional assumptions about the variables defined in our model. Let's say that $$\beta \sim Exponential(\alpha)$$, $$z \sim Poisson(\beta)$$ and $$x \sim Normal(z, \beta)$$. The generative process of this model is:

```python
beta = exponential_random(rate=alpha)
for i in (M=2):
	z_i = poisson_random(rate=beta)
	x_i = normal_random(mean=z_i, variance=beta)
```

Just as with the joint probability, this "generative story" is the one that allows us to do synthetic data generation. This synthetic data generation also allows us to check whether our model is correct or not. If we can synthesize data that resembles the observed data, then we are on a good way.

## How to learn with these models?
*Inference* is a word that daunted me a lot when I was learning about probabilistic models. However, the meaning is very simple: inference refers to learning a whole distribution based on the model you defined for your variables. This is different from *estimation* which tries to retrieve one value for each of the parameters of your model, this is also refereed to as "point estimation".

In the Bayesian setting there are two ways of doing inference: exact inference and approximate inference. Exact inference, as you might have guessed, refers to inferring the exact distribution of the variables. This can be computationally expensive for most interesting cases so we have to move to approximate inference. Approximate inference refers to looking for some form of approximation of the distribution you want to infer. A couple of famous approximation methods which most likely will  be covered in future blog posts are Markov Chain Monte Carlo (MCMC) which is based on sampling. MCMC, unfortunately, can also be computationally expensive. The second method is called variational inference which, given a family of distributions (let's say the family of all possible gaussians), aims to approximate the posterior distribution to the best distribution of the chosen family.

## Conclusions and future posts
This blog post is the first of a series of posts on Bayesian learning. Even though this is a high level explanation of what generative models are, I feel it is essential in order to write more advanced posts with a consistent story. In future posts on the generative models series I would like to write about approximate inference methods, the relation between deep learning and generative models, and some applications of these models to my own research.

### REFERENCES
<ol class="bibliography"><li><span id="goodfellow_dl_2016">Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). <i>Deep Learning</i>. MIT Press. Retrieved from <a href="http://www.deeplearningbook.org" target="_blank">link</a></span></li>
<li><span id="winn_mbmlb_2015">Bishop, C., Winn, J., &amp; Diethe, T. (2015). <i>Model-Based Machine Learning</i>. Retrieved from <a href="http://www.mbmlbook.com/index.htmlg" target="_blank">link</a></span></li>
<li><span id="adams_mlss_2018">Adams, R. (2018). A Tutorial on Deep Probabilistic Generative Models. Machine Learning Summer School. Retrieved from <a href="http://mlss2018.net.ar/slides/Adams-1.pdf" target="_blank">link</a></span></li>
<li><span id="pereira_mbml_2019">Pereira, F., &amp; Rodrigues, F. (2019). Model Based Machine Learning. Danmarks Tekniske Universitet. Retrieved from <a href="https://github.com/Chechgm/42186-model-based-machine-learning" target="_blank">(Unofficial) repo link</a></span></li>
<li><span id="zhang_variational_2017">Advances in Variational Inference. (2017). <i>ArXiv e-Prints</i>, arXiv:1711.05597. Retrieved from <a href="https://arxiv.org/pdf/1711.05597.pdf" target="_blank">link</a></span></li>
<li><span id="brock_biggan_2018">Brock, A., Donahue, J., &amp; Simonyan, K. (2018). Large Scale GAN Training for High Fidelity Natural Image Synthesis. <i>ArXiv e-Prints</i>, arXiv:1809.11096. Retrieved from <a href="https://arxiv.org/pdf/1809.11096.pdf" target="_blank">link</a></span></li>
<li><span id="tipping_ppca_1999">Tipping, M. E., &amp; Bishop, C. M. (1999). Probabilistic Principal Component Analysis. <i>JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B</i>, <i>61</i>(3), 611–622. Retrieved from <a href="http://www.robots.ox.ac.uk/ cvrg/hilary2006/ppca.pdf" target="_blank">link</a></span></li>
<li><span id="ghahramani_dbn_1998">Ghahramani, Z. (1998). Learning dynamic Bayesian networks. In <i>Adaptive Processing of Sequences and Data Structures</i> (pp. 168–197). Springer Berlin Heidelberg. Retrieved from <a href="http://mlg.eng.cam.ac.uk/zoubin/papers/vietri.pdf" target="_blank">link</a></span></li>
<li><span id="hoffman_svi_2013">Hoffman, M. D., Blei, D. M., Wang, C., &amp; Paisley, J. (2013). Stochastic Variational Inference. <i>Journal of Machine Learning Research</i>, <i>14</i>, 1303–1347. Retrieved from <a href="http://jmlr.org/papers/volume14/hoffman13a/hoffman13a.pdf" target="_blank">link</a></span></li></ol>


#### Thanks
I would like to thank Katarzyna Zukowska, Filipe Rodrigues, Mateo Dulce and Aldo Pareja for reading this post and their insightful comments.
